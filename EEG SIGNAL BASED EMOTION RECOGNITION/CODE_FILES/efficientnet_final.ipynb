{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "frqW9yhgPLRE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.signal import butter, lfilter\n",
        "import pywt\n",
        "\n",
        "# Load DEAP dataset\n",
        "def load_deap_data(data_dir):\n",
        "    eeg_data = []\n",
        "    labels = []\n",
        "\n",
        "    for file in os.listdir(data_dir):\n",
        "        if file.endswith('.dat'):\n",
        "            file_path = os.path.join(data_dir, file)\n",
        "            with open(file_path, 'rb') as f:\n",
        "                subject_data = pickle.load(f, encoding='latin1')\n",
        "                eeg_data.append(subject_data['data'])\n",
        "                labels.append(subject_data['labels'])\n",
        "\n",
        "    eeg_data = np.concatenate(eeg_data, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return eeg_data, labels\n",
        "\n",
        "# Preprocess data: normalize, filter, and extract delta rhythm\n",
        "def preprocess_data(eeg_data, lowcut=0.5, highcut=4.0, fs=128, order=5):\n",
        "    def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "        nyquist = 0.5 * fs\n",
        "        low = lowcut / nyquist\n",
        "        high = highcut / nyquist\n",
        "        b, a = butter(order, [low, high], btype='band')\n",
        "        return b, a\n",
        "\n",
        "    def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = lfilter(b, a, data)\n",
        "        return y\n",
        "\n",
        "    # Normalizing the data\n",
        "    eeg_data = eeg_data / np.max(np.abs(eeg_data), axis=(1, 2), keepdims=True)\n",
        "    filtered_data = []\n",
        "    for trial in eeg_data:\n",
        "        filtered_trial = np.array([bandpass_filter(channel, lowcut, highcut, fs, order) for channel in trial])\n",
        "        filtered_data.append(filtered_trial)\n",
        "    return np.array(filtered_data)\n",
        "\n",
        "# Convert EEG signals to images using Continuous Wavelet Transform (CWT)\n",
        "def eeg_to_cwt_images(eeg_data, scales, waveletname='morl'):\n",
        "    eeg_images = []\n",
        "    for trial in eeg_data:\n",
        "        trial_images = []\n",
        "        for channel in trial:\n",
        "            coeffs, _ = pywt.cwt(channel, scales, waveletname)\n",
        "            trial_images.append(coeffs)\n",
        "        trial_images = np.stack(trial_images[:3], axis=-1)  # Stacking first 3 channels along the last dimension\n",
        "        eeg_images.append(trial_images)\n",
        "    return np.array(eeg_images)\n",
        "\n",
        "# Label the dataset into four classes based on valence and arousal\n",
        "def label_data(labels):\n",
        "    valence = labels[:, 0]\n",
        "    arousal = labels[:, 1]\n",
        "    combined_labels = []\n",
        "    for v, a in zip(valence, arousal):\n",
        "        if v >= 5 and a >= 5:\n",
        "            combined_labels.append('Excited')\n",
        "        elif v >= 5 and a < 5:\n",
        "            combined_labels.append('Calm')\n",
        "        elif v < 5 and a >= 5:\n",
        "            combined_labels.append('Stressed')\n",
        "        else:\n",
        "            combined_labels.append('Sad')\n",
        "    return np.array(combined_labels)\n",
        "\n",
        "# Generator function to yield batches of data\n",
        "def data_generator(eeg_data, labels, scales, batch_size, augment_fn=None):\n",
        "    num_samples = eeg_data.shape[0]\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_data = eeg_data[offset:offset + batch_size]\n",
        "            batch_labels = labels[offset:offset + batch_size]\n",
        "            batch_images = eeg_to_cwt_images(batch_data, scales)\n",
        "            batch_images_resized = np.array([tf.image.resize(img, (224, 224)).numpy() for img in batch_images])\n",
        "            if batch_images_resized.shape[-1] != 3:\n",
        "                batch_images_rgb = np.repeat(batch_images_resized[..., np.newaxis], 3, axis=-1)\n",
        "            else:\n",
        "                batch_images_rgb = batch_images_resized\n",
        "            if augment_fn:\n",
        "                batch_images_rgb = augment_fn(batch_images_rgb)\n",
        "            yield batch_images_rgb, batch_labels\n",
        "\n",
        "# Load and preprocess the data\n",
        "data_dir = '/content/drive/My Drive/data_preprocessed_python'  # Update this path to your dataset in Google Drive\n",
        "eeg_data, labels = load_deap_data(data_dir)\n",
        "eeg_data = preprocess_data(eeg_data)\n",
        "scales = np.arange(1, 129)\n",
        "\n",
        "# Label the dataset\n",
        "combined_labels = label_data(labels)\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(combined_labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(eeg_data, encoded_labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Data Augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkpGORSYQFWR",
        "outputId": "7985ee7a-2f4b-4b24-8b3c-5d5daece34f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n",
            "Epoch 1/8\n",
            "128/128 [==============================] - 6891s 54s/step - loss: 1.8914 - accuracy: 0.2613 - val_loss: 1.9001 - val_accuracy: 0.3125\n",
            "Epoch 2/8\n",
            "128/128 [==============================] - 6970s 54s/step - loss: 1.6392 - accuracy: 0.2829 - val_loss: 1.7723 - val_accuracy: 0.3356\n",
            "Epoch 3/8\n",
            "128/128 [==============================] - 7212s 56s/step - loss: 1.4176 - accuracy: 0.3006 - val_loss: 1.7235 - val_accuracy: 0.3329\n",
            "Epoch 4/8\n",
            "128/128 [==============================] - 7342s 57s/step - loss: 1.3861 - accuracy: 0.3423 - val_loss: 1.4214 - val_accuracy: 0.3554\n",
            "Epoch 5/8\n",
            "128/128 [==============================] - 7347s 57s/step - loss: 1.3692 - accuracy: 0.3651 - val_loss: 1.3339 - val_accuracy: 0.3600\n",
            "Epoch 6/8\n",
            "128/128 [==============================] - 7118s 55s/step - loss: 1.3411 - accuracy: 0.3709 - val_loss: 1.3217 - val_accuracy: 0.3871\n",
            "Epoch 7/8\n",
            "128/128 [==============================] - 7424s 58s/step - loss: 1.3218 - accuracy: 0.3865 - val_loss: 1.3173 - val_accuracy: 0.3919\n",
            "Epoch 8/8\n",
            "128/128 [==============================] - 7361s 57s/step - loss: 1.3024 - accuracy: 0.3970 - val_loss: 1.3050 - val_accuracy: 0.3926\n",
            "Epoch 1/4\n",
            "128/128 [==============================] - 10818s 84s/step - loss: 1.2899 - accuracy: 0.4119 - val_loss: 1.3012 - val_accuracy: 0.4073\n",
            "Epoch 2/4\n",
            "128/128 [==============================] – 11,576s 90s/step - loss: 1.2781 - accuracy: 0.4276 - val_loss: 1.2796 - val_accuracy: 0.4129\n",
            "Epoch 3/4\n",
            "128/128 [==============================] – 12,054s 94s/step - loss: 1.2114 - accuracy: 0.4231 - val_loss: 1.1698 - val_accuracy: 0.4236\n",
            "Epoch 4/4\n",
            "128/128 [==============================] – 11,592s 90s/step - loss: 1.1123 - accuracy: 0.4289 - val_loss: 1.1195 - val_accuracy: 0.4225\n",
            "Accuracy: 0.4015\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.signal import butter, lfilter\n",
        "# Build the custom model using EfficientNet\n",
        "base_model = EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data generator\n",
        "batch_size = 4\n",
        "train_generator = data_generator(X_train, y_train, scales, batch_size, augment_fn=data_augmentation)\n",
        "test_generator = data_generator(X_test, y_test, scales, batch_size)\n",
        "val_generator = data_generator(X_val, y_val, scales, batch_size)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "validation_steps = len(X_test) // batch_size\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(train_generator, epochs=8, steps_per_epoch=steps_per_epoch,\n",
        "                    validation_data=val_generator, validation_steps=validation_steps, callbacks=[early_stopping])\n",
        "\n",
        "# Fine-tune the model\n",
        "base_model.trainable = True\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_finetune = model.fit(train_generator, epochs=4, steps_per_epoch=steps_per_epoch,\n",
        "                             validation_data=val_generator, validation_steps=validation_steps, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = model.evaluate(test_generator, steps=len(X_test) // batch_size)\n",
        "print(\"Accuracy:\", test_accuracy)\n",
        "\n",
        "\n",
        "# Print epochs and train/validation accuracy and loss\n",
        "def print_epoch_metrics(history, history_finetune=None):\n",
        "    print(\"Epoch\\tTrain Loss\\tTrain Accuracy\\tValidation Loss\\tValidation Accuracy\")\n",
        "    for i in range(len(history.history['loss'])):\n",
        "        print(f\"{i+1}\\t{history.history['loss'][i]:.4f}\\t\\t{history.history['accuracy'][i]:.4f}\\t\\t{history.history['val_loss'][i]:.4f}\\t\\t{history.history['val_accuracy'][i]:.4f}\")\n",
        "    if history_finetune:\n",
        "        for i in range(len(history_finetune.history['loss'])):\n",
        "            print(f\"{i+1 + len(history.history['loss'])}\\t{history_finetune.history['loss'][i]:.4f}\\t\\t{history_finetune.history['accuracy'][i]:.4f}\\t\\t{history_finetune.history['val_loss'][i]:.4f}\\t\\t{history_finetune.history['val_accuracy'][i]:.4f}\")\n",
        "\n",
        "print_epoch_metrics(history, history_finetune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj1MEIWBe7Gt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
